{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a947bea-1f41-4824-b47c-ba0adfca67e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Trips and Users\n",
    "\n",
    "Write a SQL query to find the cancellation rate of requests with unbanned users\n",
    "(both client and driver must not be banned) each day between \"2013-10-01\" and \"2013-10-03\".\n",
    "Round Cancellation Rate to two decimal points.\n",
    "\n",
    "The cancellation rate is computed by dividing the number of canceled (by client or driver)\n",
    "requests with unbanned users by the total number of requests with unbanned users on that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdfe726a-9945-4d58-926a-7f6a235f6d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Switch to my Catalog\n",
    "USE CATALOG workspace;\n",
    "\n",
    "-- Create schema if not exists\n",
    "CREATE SCHEMA IF NOT EXISTS sql_pyspark_practice;\n",
    "\n",
    "-- Use this schema\n",
    "USE sql_pyspark_practice;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c296c806-88ac-49ef-8ea9-84cd1764ef06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cee59e3-5189-48ac-9486-870b474b6d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "Create table  Trips (id int, client_id int, driver_id int, city_id int, status varchar(50), request_at varchar(50));\n",
    "\n",
    "Create table Users (users_id int, banned varchar(50), role varchar(50));\n",
    "\n",
    "insert into Trips (id, client_id, driver_id, city_id, status, request_at) values ('1', '1', '10', '1', 'completed', '2013-10-01');\n",
    "insert into Trips (id, client_id, driver_id, city_id, status, request_at) values ('2', '2', '11', '1', 'cancelled_by_driver', '2013-10-01');\n",
    "insert into Trips (id, client_id, driver_id, city_id, status, request_at) values ('3', '3', '12', '6', 'completed', '2013-10-01');\n",
    "insert into Trips (id, client_id, driver_id, city_id, status, request_at) values ('4', '4', '13', '6', 'cancelled_by_client', '2013-10-01');\n",
    "insert into Trips (id, client_id, driver_id, city_id, status, request_at) values ('5', '1', '10', '1', 'completed', '2013-10-02');\n",
    "insert into Trips (id, client_id, driver_id, city_id, status, request_at) values ('6', '2', '11', '6', 'completed', '2013-10-02');\n",
    "insert into Trips (id, client_id, driver_id, city_id, status, request_at) values ('7', '3', '12', '6', 'completed', '2013-10-02');\n",
    "insert into Trips (id, client_id, driver_id, city_id, status, request_at) values ('8', '2', '12', '12', 'completed', '2013-10-03');\n",
    "insert into Trips (id, client_id, driver_id, city_id, status, request_at) values ('9', '3', '10', '12', 'completed', '2013-10-03');\n",
    "insert into Trips (id, client_id, driver_id, city_id, status, request_at) values ('10', '4', '13', '12', 'cancelled_by_driver', '2013-10-03');\n",
    "\n",
    "\n",
    "insert into Users (users_id, banned, role) values ('1', 'No', 'client');\n",
    "insert into Users (users_id, banned, role) values ('2', 'Yes', 'client');\n",
    "insert into Users (users_id, banned, role) values ('3', 'No', 'client');\n",
    "insert into Users (users_id, banned, role) values ('4', 'No', 'client');\n",
    "insert into Users (users_id, banned, role) values ('10', 'No', 'driver');\n",
    "insert into Users (users_id, banned, role) values ('11', 'No', 'driver');\n",
    "insert into Users (users_id, banned, role) values ('12', 'No', 'driver');\n",
    "insert into Users (users_id, banned, role) values ('13', 'No', 'driver');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c4fdb8e-f80a-420a-8c87-86ede664d2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from trips;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a92526-af64-4366-a999-6e873de1b105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from users;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "205e4d83-df18-4e2f-adc7-b908e9941ee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT \n",
    "    request_at,\n",
    "    ROUND(\n",
    "        COUNT(CASE WHEN status IN ('cancelled_by_client','cancelled_by_driver') THEN 1 END)\n",
    "        / COUNT(*) * 100,\n",
    "    2) AS cancellation_rate\n",
    "FROM Trips t\n",
    "JOIN Users c ON t.client_id = c.users_id\n",
    "JOIN Users d ON t.driver_id = d.users_id\n",
    "WHERE c.banned = 'No' \n",
    "  AND d.banned = 'No'\n",
    "  AND request_at BETWEEN '2013-10-01' AND '2013-10-03'\n",
    "GROUP BY request_at;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec74b999-3947-430e-907b-877ecf880558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "trips_df = spark.table(\"trips\")\n",
    "users_df = spark.table(\"users\")\n",
    "# Alias Users dataframe for client and driver\n",
    "clients_df = users_df.alias(\"c\")\n",
    "drivers_df = users_df.alias(\"d\")\n",
    "trips = trips_df.alias(\"t\")\n",
    "\n",
    "# Join trips with client and driver users\n",
    "joined = (\n",
    "    trips\n",
    "    .join(clients_df, F.col(\"t.client_id\") == F.col(\"c.users_id\"))\n",
    "    .join(drivers_df, F.col(\"t.driver_id\") == F.col(\"d.users_id\"))\n",
    ")\n",
    "\n",
    "# Filter: non-banned client & driver, date range filter\n",
    "filtered = (\n",
    "    joined\n",
    "    .where(F.col(\"c.banned\") == \"No\")\n",
    "    .where(F.col(\"d.banned\") == \"No\")\n",
    "    .where((F.col(\"t.request_at\") >= \"2013-10-01\") & (F.col(\"t.request_at\") <= \"2013-10-03\"))\n",
    ")\n",
    "\n",
    "# Aggregate by date\n",
    "result = (\n",
    "    filtered\n",
    "    .groupBy(\"t.request_at\")\n",
    "    .agg(\n",
    "        # Count of cancelled trips\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                F.col(\"t.status\").isin(\"cancelled_by_client\", \"cancelled_by_driver\"), \n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).alias(\"cancelled_trips\"),\n",
    "        # Total trips\n",
    "        F.count(F.lit(1)).alias(\"total_trips\")\n",
    "    )\n",
    "    # Compute cancellation rate and round\n",
    "    .withColumn(\n",
    "        \"cancellation_rate\",\n",
    "        F.round(F.col(\"cancelled_trips\") / F.col(\"total_trips\") * 100, 2)\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"t.request_at\").alias(\"request_at\"),\n",
    "        \"cancellation_rate\"\n",
    "    )\n",
    "    .orderBy(\"request_at\")\n",
    ")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ace921f2-8858-4dbe-881a-5730a7bc5c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1â€“3ï¸âƒ£ What was the original question?\n",
    "\n",
    "### Given SQL:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "    request_at,\n",
    "    ROUND(\n",
    "        COUNT(CASE WHEN status IN ('cancelled_by_client','cancelled_by_driver') THEN 1 END)\n",
    "        / COUNT(*) * 100,\n",
    "    2) AS cancellation_rate\n",
    "FROM Trips t\n",
    "JOIN Users c ON t.client_id = c.users_id\n",
    "JOIN Users d ON t.driver_id = d.users_id\n",
    "WHERE c.banned = 'No' \n",
    "  AND d.banned = 'No'\n",
    "  AND request_at BETWEEN '2013-10-01' AND '2013-10-03'\n",
    "GROUP BY request_at;\n",
    "```\n",
    "\n",
    "From this, we can reconstruct something like this (this is essentially the original LeetCode â€œTrips and Usersâ€ question):\n",
    "\n",
    "### Reconstructed Problem (in plain English)\n",
    "\n",
    "We have two tables:\n",
    "\n",
    "1. **Trips**\n",
    "\n",
    "   * Contains ride requests between clients and drivers.\n",
    "   * Has columns like:\n",
    "\n",
    "     * `client_id` â€“ which user requested the ride\n",
    "     * `driver_id` â€“ which user drove the ride\n",
    "     * `status` â€“ status of the trip (e.g., `'completed'`, `'cancelled_by_driver'`, `'cancelled_by_client'`)\n",
    "     * `request_at` â€“ the date the trip was requested\n",
    "\n",
    "2. **Users**\n",
    "\n",
    "   * Contains information about each user (both drivers and clients).\n",
    "   * Has columns like:\n",
    "\n",
    "     * `users_id` â€“ unique user ID\n",
    "     * `banned` â€“ whether the user is banned (`'Yes'` / `'No'`)\n",
    "     * `role` â€“ `'client'` or `'driver'`\n",
    "\n",
    "**Question:**\n",
    "\n",
    "> For each day between **2013-10-01** and **2013-10-03** (inclusive),\n",
    "> calculate the **cancellation rate of trips** made by **non-banned clients and non-banned drivers**.\n",
    ">\n",
    "> The **cancellation rate** for a date is:\n",
    ">\n",
    "> [\n",
    "> \\text{cancellation rate} = \\frac{\\text{number of cancelled trips on that date}}{\\text{total trips on that date}} \\times 100\n",
    "> ]\n",
    ">\n",
    "> A trip is considered **cancelled** if `status` is `'cancelled_by_client'` or `'cancelled_by_driver'`.\n",
    "> Return the result as:\n",
    ">\n",
    "> * `request_at` (the date)\n",
    "> * `cancellation_rate` (rounded to 2 decimal places)\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Why does this problem matter? What concepts is it testing?\n",
    "\n",
    "This question is testing core **data analysis + SQL aggregation** skills:\n",
    "\n",
    "1. **JOINs and table relationships**\n",
    "\n",
    "   * You must realize that `Trips` references `Users` twice: once as client, once as driver.\n",
    "   * You need to join the `Users` table **twice** with different aliases.\n",
    "\n",
    "2. **Filtering rows by conditions**\n",
    "\n",
    "   * Exclude banned users (`banned = 'No'` for both client and driver).\n",
    "   * Restrict to a **date range** (`request_at BETWEEN '2013-10-01' AND '2013-10-03'`).\n",
    "\n",
    "3. **Conditional aggregation**\n",
    "\n",
    "   * Using `COUNT(CASE WHEN ... THEN 1 END)` to count only **cancelled** trips.\n",
    "   * Using `COUNT(*)` for total trips.\n",
    "\n",
    "4. **Grouping**\n",
    "\n",
    "   * Grouping by date (`GROUP BY request_at`) to calculate metrics **per day**.\n",
    "\n",
    "5. **Numeric computation**\n",
    "\n",
    "   * Dividing one aggregate by another and multiplying by 100.\n",
    "   * Using `ROUND(..., 2)` to round to 2 decimal places.\n",
    "   * Awareness of integer vs decimal division.\n",
    "\n",
    "These are exactly the kind of skills you need for **analytics interviews**, **LeetCode SQL**, and **real-world reporting queries**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ How to think about solving this type of problem (general methodology)\n",
    "\n",
    "When you get a problem like:\n",
    "\n",
    "> â€œCompute some rate/percentage per day for some subset of users/rowsâ€¦â€\n",
    "\n",
    "You can think like this:\n",
    "\n",
    "1. **Understand the tables and relationships**\n",
    "\n",
    "   * Which table contains the events (trips)? â†’ `Trips`\n",
    "   * Which table contains attributes about users? â†’ `Users`\n",
    "   * How are they linked? â†’ `Trips.client_id = Users.users_id`, `Trips.driver_id = Users.users_id`\n",
    "\n",
    "2. **Clarify who/what to include**\n",
    "\n",
    "   * Only trips with non-banned clients and drivers.\n",
    "   * Only trips within a date range.\n",
    "   * Only certain statuses count as \"cancelled\".\n",
    "\n",
    "3. **Decide grouping keys**\n",
    "\n",
    "   * What is the â€œper Xâ€ in the question?\n",
    "     â†’ Here itâ€™s **per day** = group by `request_at`.\n",
    "\n",
    "4. **Decide metrics to compute**\n",
    "\n",
    "   * **Total trips per day**: `COUNT(*)`\n",
    "   * **Cancelled trips per day**: `COUNT(CASE WHEN status IN (...) THEN 1 END)`\n",
    "   * **Rate**: `cancelled / total * 100`, rounded.\n",
    "\n",
    "5. **Structure the query**\n",
    "\n",
    "   * FROM base table (Trips)\n",
    "   * JOIN the tables needed (Users twice)\n",
    "   * WHERE filters\n",
    "   * GROUP BY date\n",
    "   * SELECT date + computed metric\n",
    "\n",
    "Once you internalize this pattern, you can reuse it for:\n",
    "\n",
    "* completion rate\n",
    "* click-through rate\n",
    "* churn rate\n",
    "* conversion rate\n",
    "  â€¦anything that is â€œnumber of X / total Y per groupâ€.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Break the SQL solution into steps\n",
    "\n",
    "Letâ€™s dissect the SQL:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "    request_at,\n",
    "    ROUND(\n",
    "        COUNT(CASE WHEN status IN ('cancelled_by_client','cancelled_by_driver') THEN 1 END)\n",
    "        / COUNT(*) * 100,\n",
    "    2) AS cancellation_rate\n",
    "FROM Trips t\n",
    "JOIN Users c ON t.client_id = c.users_id\n",
    "JOIN Users d ON t.driver_id = d.users_id\n",
    "WHERE c.banned = 'No' \n",
    "  AND d.banned = 'No'\n",
    "  AND request_at BETWEEN '2013-10-01' AND '2013-10-03'\n",
    "GROUP BY request_at;\n",
    "```\n",
    "\n",
    "### Step-by-step:\n",
    "\n",
    "#### Step 1: FROM and JOINs\n",
    "\n",
    "```sql\n",
    "FROM Trips t\n",
    "JOIN Users c ON t.client_id = c.users_id\n",
    "JOIN Users d ON t.driver_id = d.users_id\n",
    "```\n",
    "\n",
    "* `Trips t`: base table, one row per trip.\n",
    "* `Users c`: join to get info about the **client** (`c` = client).\n",
    "* `Users d`: join to get info about the **driver** (`d` = driver).\n",
    "* We now have each trip with:\n",
    "\n",
    "  * trip data (`t.*`)\n",
    "  * client attributes (`c.*`)\n",
    "  * driver attributes (`d.*`)\n",
    "\n",
    "#### Step 2: Filter banned users and date range\n",
    "\n",
    "```sql\n",
    "WHERE c.banned = 'No' \n",
    "  AND d.banned = 'No'\n",
    "  AND request_at BETWEEN '2013-10-01' AND '2013-10-03'\n",
    "```\n",
    "\n",
    "* `c.banned = 'No'` â†’ only trips where the **client** is not banned.\n",
    "* `d.banned = 'No'` â†’ only trips where the **driver** is not banned.\n",
    "* `request_at BETWEEN '2013-10-01' AND '2013-10-03'`\n",
    "  â†’ only trips requested within those 3 days.\n",
    "\n",
    "After this WHERE, the rows weâ€™re grouping are exactly the trips we care about.\n",
    "\n",
    "#### Step 3: Grouping by date\n",
    "\n",
    "```sql\n",
    "GROUP BY request_at;\n",
    "```\n",
    "\n",
    "* We want one row **per day**.\n",
    "* So for each unique `request_at` (date), we compute aggregates.\n",
    "\n",
    "#### Step 4: Counting cancelled vs total trips\n",
    "\n",
    "Inside `SELECT`:\n",
    "\n",
    "```sql\n",
    "COUNT(CASE WHEN status IN ('cancelled_by_client','cancelled_by_driver') THEN 1 END)\n",
    "```\n",
    "\n",
    "* `CASE` is used only to count certain rows.\n",
    "* If `status` is one of the cancelled statuses â†’ returns `1`, so `COUNT(1)` counts it.\n",
    "* If status is something else (e.g., `'completed'`), `CASE` returns `NULL`; `COUNT(NULL)` ignores it.\n",
    "* So this gives: **cancelled trip count per day**.\n",
    "\n",
    "```sql\n",
    "COUNT(*) \n",
    "```\n",
    "\n",
    "* Counts **all trips** (non-filtered, within this group).\n",
    "* This is the **total trip count per day**.\n",
    "\n",
    "#### Step 5: Computing the percentage and rounding\n",
    "\n",
    "```sql\n",
    "ROUND(\n",
    "    COUNT(CASE WHEN status IN ('cancelled_by_client','cancelled_by_driver') THEN 1 END)\n",
    "    / COUNT(*) * 100,\n",
    "2) AS cancellation_rate\n",
    "```\n",
    "\n",
    "* Numerator: number of cancelled trips per day.\n",
    "* Denominator: total trips per day.\n",
    "* Multiply by 100 to make it a percentage.\n",
    "* `ROUND(..., 2)` rounds to 2 decimal places.\n",
    "* aliased as `cancellation_rate`.\n",
    "\n",
    "> âš ï¸ Practical detail: in some SQL engines, `COUNT(...) / COUNT(*)` will do **integer division**.\n",
    "> To be safe you might write:\n",
    ">\n",
    "> ```sql\n",
    "> 1.0 * COUNT(...) / COUNT(*) * 100\n",
    "> ```\n",
    ">\n",
    "> or cast one side to `DECIMAL`. But conceptually, your logic is correct.\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Rewrite the SQL in a cleaner, commented way\n",
    "\n",
    "```sql\n",
    "-- Calculate daily cancellation rate for trips between 2013-10-01 and 2013-10-03\n",
    "-- considering only trips where BOTH client and driver are not banned.\n",
    "SELECT\n",
    "    t.request_at,  -- date on which the trip was requested\n",
    "\n",
    "    -- Cancellation rate = (cancelled trips / total trips) * 100\n",
    "    ROUND(\n",
    "        100.0 * \n",
    "        COUNT(\n",
    "            CASE \n",
    "                WHEN t.status IN ('cancelled_by_client', 'cancelled_by_driver') \n",
    "                THEN 1 \n",
    "            END\n",
    "        ) / COUNT(*),\n",
    "        2\n",
    "    ) AS cancellation_rate\n",
    "FROM Trips t\n",
    "    -- Join users table as client\n",
    "    JOIN Users c \n",
    "        ON t.client_id = c.users_id\n",
    "    -- Join users table as driver\n",
    "    JOIN Users d \n",
    "        ON t.driver_id = d.users_id\n",
    "WHERE \n",
    "    c.banned = 'No'                -- client not banned\n",
    "    AND d.banned = 'No'            -- driver not banned\n",
    "    AND t.request_at BETWEEN '2013-10-01' AND '2013-10-03'\n",
    "GROUP BY \n",
    "    t.request_at\n",
    "ORDER BY \n",
    "    t.request_at;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ Equivalent PySpark DataFrame code\n",
    "\n",
    "Assume you already have two DataFrames:\n",
    "\n",
    "* `trips_df` for `Trips`\n",
    "* `users_df` for `Users`\n",
    "\n",
    "Weâ€™ll do the same logic in PySpark:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Alias Users dataframe for client and driver\n",
    "clients_df = users_df.alias(\"c\")\n",
    "drivers_df = users_df.alias(\"d\")\n",
    "trips = trips_df.alias(\"t\")\n",
    "\n",
    "# Join trips with client and driver users\n",
    "joined = (\n",
    "    trips\n",
    "    .join(clients_df, F.col(\"t.client_id\") == F.col(\"c.users_id\"))\n",
    "    .join(drivers_df, F.col(\"t.driver_id\") == F.col(\"d.users_id\"))\n",
    ")\n",
    "\n",
    "# Filter: non-banned client & driver, date range filter\n",
    "filtered = (\n",
    "    joined\n",
    "    .where(F.col(\"c.banned\") == \"No\")\n",
    "    .where(F.col(\"d.banned\") == \"No\")\n",
    "    .where((F.col(\"t.request_at\") >= \"2013-10-01\") & (F.col(\"t.request_at\") <= \"2013-10-03\"))\n",
    ")\n",
    "\n",
    "# Aggregate by date\n",
    "result = (\n",
    "    filtered\n",
    "    .groupBy(\"t.request_at\")\n",
    "    .agg(\n",
    "        # Count of cancelled trips\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                F.col(\"t.status\").isin(\"cancelled_by_client\", \"cancelled_by_driver\"), \n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).alias(\"cancelled_trips\"),\n",
    "        # Total trips\n",
    "        F.count(F.lit(1)).alias(\"total_trips\")\n",
    "    )\n",
    "    # Compute cancellation rate and round\n",
    "    .withColumn(\n",
    "        \"cancellation_rate\",\n",
    "        F.round(F.col(\"cancelled_trips\") / F.col(\"total_trips\") * 100, 2)\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"t.request_at\").alias(\"request_at\"),\n",
    "        \"cancellation_rate\"\n",
    "    )\n",
    "    .orderBy(\"request_at\")\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Explain the PySpark code line by line\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "```\n",
    "\n",
    "* Import PySpark SQL functions with alias `F` (common convention).\n",
    "\n",
    "```python\n",
    "clients_df = users_df.alias(\"c\")\n",
    "drivers_df = users_df.alias(\"d\")\n",
    "trips = trips_df.alias(\"t\")\n",
    "```\n",
    "\n",
    "* We create **aliases** for the same `users_df` DataFrame:\n",
    "\n",
    "  * `\"c\"` for clients\n",
    "  * `\"d\"` for drivers\n",
    "* Alias `trips_df` as `\"t\"` to mimic SQL table aliases.\n",
    "\n",
    "```python\n",
    "joined = (\n",
    "    trips\n",
    "    .join(clients_df, F.col(\"t.client_id\") == F.col(\"c.users_id\"))\n",
    "    .join(drivers_df, F.col(\"t.driver_id\") == F.col(\"d.users_id\"))\n",
    ")\n",
    "```\n",
    "\n",
    "* Join `Trips` with `Users` for client, then join again for driver.\n",
    "* Same logic as SQL `JOIN Users c ON ...` and `JOIN Users d ON ...`.\n",
    "\n",
    "```python\n",
    "filtered = (\n",
    "    joined\n",
    "    .where(F.col(\"c.banned\") == \"No\")\n",
    "    .where(F.col(\"d.banned\") == \"No\")\n",
    "    .where((F.col(\"t.request_at\") >= \"2013-10-01\") & (F.col(\"t.request_at\") <= \"2013-10-03\"))\n",
    ")\n",
    "```\n",
    "\n",
    "* Add filters:\n",
    "\n",
    "  * client not banned\n",
    "  * driver not banned\n",
    "  * request_at between the two dates\n",
    "* Equivalent to SQL `WHERE` clause.\n",
    "\n",
    "```python\n",
    "result = (\n",
    "    filtered\n",
    "    .groupBy(\"t.request_at\")\n",
    "    .agg(\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                F.col(\"t.status\").isin(\"cancelled_by_client\", \"cancelled_by_driver\"), \n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).alias(\"cancelled_trips\"),\n",
    "        F.count(F.lit(1)).alias(\"total_trips\")\n",
    "    )\n",
    "```\n",
    "\n",
    "* `groupBy(\"t.request_at\")` â†’ group rows by `request_at` date.\n",
    "* Inside `.agg(...)` we calculate:\n",
    "\n",
    "  * `cancelled_trips`:\n",
    "\n",
    "    * `F.when(condition, 1).otherwise(0)` gives **1 or 0 per row**.\n",
    "    * `F.sum(...)` totals them per group.\n",
    "  * `total_trips`:\n",
    "\n",
    "    * `F.count(F.lit(1))` counts all rows in the group (like `COUNT(*)`).\n",
    "\n",
    "```python\n",
    "    .withColumn(\n",
    "        \"cancellation_rate\",\n",
    "        F.round(F.col(\"cancelled_trips\") / F.col(\"total_trips\") * 100, 2)\n",
    "    )\n",
    "```\n",
    "\n",
    "* Create a new column `cancellation_rate` = (cancelled / total * 100), rounded to 2 decimals.\n",
    "\n",
    "```python\n",
    "    .select(\n",
    "        F.col(\"t.request_at\").alias(\"request_at\"),\n",
    "        \"cancellation_rate\"\n",
    "    )\n",
    "    .orderBy(\"request_at\")\n",
    ")\n",
    "```\n",
    "\n",
    "* Select only the relevant columns, rename the date column nicely, and order by date.\n",
    "\n",
    "At the end, `result` is a DataFrame equivalent to the SQL result.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”Ÿ How the thinking differs between SQL and PySpark\n",
    "\n",
    "**SQL mindset:**\n",
    "\n",
    "* You describe *what* you want, not *how* to compute it.\n",
    "* One big declarative statement:\n",
    "\n",
    "  * FROM â†’ JOIN â†’ WHERE â†’ GROUP BY â†’ SELECT.\n",
    "* The DB engine figures out the optimal execution plan.\n",
    "\n",
    "**PySpark mindset:**\n",
    "\n",
    "* You chain **transformations** on DataFrames.\n",
    "* Each step is a transformation: `join`, `where`, `groupBy`, `agg`, `withColumn`, `select`.\n",
    "* You explicitly build a **pipeline** of operations.\n",
    "* Execution is lazy: nothing runs until you perform an action (`show`, `collect`, `write` etc.).\n",
    "\n",
    "Conceptually they are the same operations, but:\n",
    "\n",
    "* SQL = one declarative query.\n",
    "* PySpark = sequence of transformations that logically mirror the SQL but in an API style.\n",
    "\n",
    "Once you can read/write the SQL clearly, translating to PySpark is mostly mechanic.\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£1ï¸âƒ£ Hints to solve this (without looking at the solution)\n",
    "\n",
    "### ğŸŸ¢ Basic hints\n",
    "\n",
    "1. **Hint 1 (tables):**\n",
    "   Identify the table with trips and the table with user info (banned or not). Ask: â€œHow do I know if a tripâ€™s client or driver is banned?â€\n",
    "\n",
    "2. **Hint 2 (double join):**\n",
    "   You need to use the **Users** table twice: once as client, once as driver. This will require **two joins** with different aliases.\n",
    "\n",
    "3. **Hint 3 (grouping):**\n",
    "   The answer is â€œper dayâ€, so think about grouping by the **date column** and using aggregate functions.\n",
    "\n",
    "### ğŸ”µ Advanced hints\n",
    "\n",
    "4. **Hint 4 (conditional count):**\n",
    "   To count only cancelled trips, you donâ€™t filter them out in the WHERE (or youâ€™d lose other statuses). Instead, use something like `COUNT(CASE WHEN ... THEN 1 END)`.\n",
    "\n",
    "5. **Hint 5 (rate formula):**\n",
    "   The cancellation rate is `cancelled_trips / total_trips * 100`. Both come from aggregates in the same group.\n",
    "\n",
    "6. **Hint 6 (filters placement):**\n",
    "   Be careful not to put the â€œcancelledâ€ condition in `WHERE`, or youâ€™ll only keep cancelled trips and lose the denominator. The cancelled condition belongs inside `CASE` in the aggregate.\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£2ï¸âƒ£ Final SQL and PySpark solutions (clean)\n",
    "\n",
    "### âœ… Final SQL\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    t.request_at,\n",
    "    ROUND(\n",
    "        100.0 * \n",
    "        COUNT(\n",
    "            CASE \n",
    "                WHEN t.status IN ('cancelled_by_client', 'cancelled_by_driver') \n",
    "                THEN 1 \n",
    "            END\n",
    "        ) / COUNT(*),\n",
    "        2\n",
    "    ) AS cancellation_rate\n",
    "FROM Trips t\n",
    "JOIN Users c \n",
    "    ON t.client_id = c.users_id\n",
    "JOIN Users d \n",
    "    ON t.driver_id = d.users_id\n",
    "WHERE \n",
    "    c.banned = 'No'\n",
    "    AND d.banned = 'No'\n",
    "    AND t.request_at BETWEEN '2013-10-01' AND '2013-10-03'\n",
    "GROUP BY \n",
    "    t.request_at\n",
    "ORDER BY \n",
    "    t.request_at;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Final PySpark\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Aliases\n",
    "trips = trips_df.alias(\"t\")\n",
    "clients_df = users_df.alias(\"c\")\n",
    "drivers_df = users_df.alias(\"d\")\n",
    "\n",
    "# Join trips with client and driver users\n",
    "joined = (\n",
    "    trips\n",
    "    .join(clients_df, F.col(\"t.client_id\") == F.col(\"c.users_id\"))\n",
    "    .join(drivers_df, F.col(\"t.driver_id\") == F.col(\"d.users_id\"))\n",
    ")\n",
    "\n",
    "# Filter by non-banned users and date range\n",
    "filtered = (\n",
    "    joined\n",
    "    .where(F.col(\"c.banned\") == \"No\")\n",
    "    .where(F.col(\"d.banned\") == \"No\")\n",
    "    .where(\n",
    "        (F.col(\"t.request_at\") >= \"2013-10-01\") &\n",
    "        (F.col(\"t.request_at\") <= \"2013-10-03\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Group and compute cancellation rate\n",
    "result = (\n",
    "    filtered\n",
    "    .groupBy(\"t.request_at\")\n",
    "    .agg(\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                F.col(\"t.status\").isin(\"cancelled_by_client\", \"cancelled_by_driver\"),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).alias(\"cancelled_trips\"),\n",
    "        F.count(F.lit(1)).alias(\"total_trips\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"cancellation_rate\",\n",
    "        F.round(F.col(\"cancelled_trips\") / F.col(\"total_trips\") * 100, 2)\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"t.request_at\").alias(\"request_at\"),\n",
    "        \"cancellation_rate\"\n",
    "    )\n",
    "    .orderBy(\"request_at\")\n",
    ")\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8991635101003146,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Prob_10",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
