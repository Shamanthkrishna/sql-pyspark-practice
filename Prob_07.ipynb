{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b79b87c-5bb7-4681-81c9-8763368c34a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pareto Principle\n",
    "- 80% of your sales comes from 20% of your products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a93997a9-a3b3-49ae-af6d-f5263d05fd3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Switch to my Catalog\n",
    "USE CATALOG workspace;\n",
    "\n",
    "-- Create schema if not exists\n",
    "CREATE SCHEMA IF NOT EXISTS sql_pyspark_practice;\n",
    "\n",
    "-- Use this schema\n",
    "USE sql_pyspark_practice;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1a7e43-bcb1-406a-a856-9f31294574d6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763486952199}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- display(\n",
    "--     spark.sql(\n",
    "--         \"\"\"\n",
    "--         select sum(sales) * 0.8 as discounted_sales\n",
    "--         from orders\n",
    "--         \"\"\"\n",
    "--     )\n",
    "-- )\n",
    "\n",
    "\n",
    "-- 80% --> 1837760.7771199604\n",
    "\n",
    "with product_wise_sales as (\n",
    "  select product_id, sum(sales) as product_sales\n",
    "  from orders\n",
    "  group by product_id\n",
    "), calc_sales as(\n",
    "select  product_id, product_sales,\n",
    "        sum(product_sales) over(order by product_sales desc) as running_sales,\n",
    "        0.8*sum(product_sales) over() as total_sales\n",
    "from product_wise_sales\n",
    ")\n",
    "\n",
    "select * from calc_sales\n",
    "where running_sales <= total_sales;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b161d4ab-62b2-4c96-aa5d-11e6c37b9066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "orders_df = spark.table(\"orders\")\n",
    "\n",
    "# Step 1: Compute total sales per product\n",
    "product_wise_sales = (\n",
    "    orders_df.groupBy(\"product_id\")\n",
    "             .agg(F.sum(\"sales\").alias(\"product_sales\"))\n",
    ")\n",
    "\n",
    "# Step 2: Window for running cumulative sales (sorted by sales desc)\n",
    "running_window = Window.orderBy(F.desc(\"product_sales\")) \\\n",
    "                       .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Step 3: Window for total sales (all rows)\n",
    "total_window = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "calc_sales = (\n",
    "    product_wise_sales\n",
    "        .withColumn(\"running_sales\", F.sum(\"product_sales\").over(running_window))\n",
    "        .withColumn(\"total_sales\", 0.8 * F.sum(\"product_sales\").over(total_window))\n",
    ")\n",
    "\n",
    "# Step 4: Filter products contributing to 80% of sales\n",
    "result = calc_sales.filter(F.col(\"running_sales\") <= F.col(\"total_sales\"))\n",
    "\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1788a725-b09b-4507-b7da-4e2e4074b57e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1) Deep analysis of the SQL you provided (short summary)\n",
    "\n",
    "Your SQL computes product-level sales, sorts products by sales (highest first), computes a running (cumulative) sum of those product sales, computes 80% of the total sales, and returns all products whose cumulative running sum is **≤ 80% of total sales**.\n",
    "\n",
    "This is the classic “find top items that contribute to X% of total” (Pareto / 80/20 style) problem.\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Reconstructed original problem (from the SQL logic)\n",
    "\n",
    "**Reconstructed problem:**\n",
    "“Given the `orders` table with a `product_id` and `sales` columns (each order row contains a sale amount), find the list of products that together account for the top 80% of total sales. Order products by sales descending and include products until the cumulative (running) sales reaches 80% of the grand total.”\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Problem explained in plain English\n",
    "\n",
    "You have many orders for different products. You want to know which products — when you take them from highest-selling to lower-selling — make up 80% of your total revenue. This helps you identify the small number of products that generate most of your revenue.\n",
    "\n",
    "---\n",
    "\n",
    "# 4) Why the problem matters & what concepts it tests\n",
    "\n",
    "**Why it matters**\n",
    "\n",
    "* Business: helps prioritize inventory, marketing, pricing, and account management on the products with highest impact.\n",
    "* Data analysis: identifies “vital few” vs “trivial many” (Pareto principle).\n",
    "* Operationally: helps focus resources on products that drive most revenue.\n",
    "\n",
    "**Concepts it tests**\n",
    "\n",
    "* Aggregation: grouping and summing.\n",
    "* Window functions: cumulative (running) sums and global totals.\n",
    "* Sorting and ranking.\n",
    "* Filtering after window calculations.\n",
    "* Edge-case handling (ties, equal cumulative threshold).\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Logical thinking / general methodology to solve this kind of problem\n",
    "\n",
    "1. Aggregate raw transactions to the unit you need (here: product-level total sales).\n",
    "2. Sort those units from most important to least (descending by product sales).\n",
    "3. Compute cumulative sum over that sorted order (running total).\n",
    "4. Compute the threshold value you want to compare against (e.g., 0.8 * grand_total).\n",
    "5. Select rows where cumulative sum ≤ threshold (or choose tie-handling rule).\n",
    "6. Optionally, adjust for ties or include the item that crosses the threshold depending on business needs.\n",
    "\n",
    "This pattern applies to any “top X% contributors” question (customers, SKUs, categories, channels).\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Break the SQL into steps and explain each step in detail\n",
    "\n",
    "Original SQL (condensed):\n",
    "\n",
    "```sql\n",
    "with product_wise_sales as (\n",
    "  select product_id, sum(sales) as product_sales\n",
    "  from orders\n",
    "  group by product_id\n",
    "),\n",
    "calc_sales as (\n",
    "  select product_id, product_sales,\n",
    "         sum(product_sales) over(order by product_sales desc) as running_sales,\n",
    "         0.8*sum(product_sales) over() as total_sales\n",
    "  from product_wise_sales\n",
    ")\n",
    "select * from calc_sales\n",
    "where running_sales <= total_sales;\n",
    "```\n",
    "\n",
    "Step-by-step explanation:\n",
    "\n",
    "1. `product_wise_sales` CTE:\n",
    "\n",
    "   * `select product_id, sum(sales) as product_sales from orders group by product_id`\n",
    "   * Purpose: collapse `orders` to one row per `product_id` with its total sales across all orders.\n",
    "\n",
    "2. `calc_sales` CTE:\n",
    "\n",
    "   * `sum(product_sales) over(order by product_sales desc) as running_sales`\n",
    "\n",
    "     * This is a window function computing a running (cumulative) sum of `product_sales` as we move down rows ordered by `product_sales` descending. Each row’s `running_sales` is the sum of product_sales from the highest-selling product down to that row.\n",
    "     * Important: if multiple products have the same `product_sales`, behavior depends on DB specifics — the cumulative sum will include all rows up to current row in the ORDER BY ordering; tie ordering may be arbitrary unless extra tie-breaker columns are specified.\n",
    "   * `0.8*sum(product_sales) over() as total_sales`\n",
    "\n",
    "     * `sum(product_sales) over()` with an empty `OVER()` computes the global sum (same value repeated on every row). Multiplying by 0.8 yields the 80% threshold. (Equivalent to: `0.8 * (select sum(product_sales) from product_wise_sales)`.)\n",
    "   * The `from product_wise_sales` provides `product_id` and `product_sales`.\n",
    "\n",
    "3. Final SELECT & filter:\n",
    "\n",
    "   * `select * from calc_sales where running_sales <= total_sales;`\n",
    "   * This returns every product whose cumulative running sum is less than or equal to the 80% threshold.\n",
    "\n",
    "Note: This logic includes products as long as the cumulative total up to that product remains ≤ 80%. The item that first pushes running_sales above 80% is excluded (because `<=`), which may be desired or not depending on business rule. An alternative is to include the product that causes cumulative >= threshold if you want to ensure at least 80% coverage.\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Rewritten SQL solution — clean, readable, commented\n",
    "\n",
    "```sql\n",
    "-- Find products that cumulatively make up 80% of total sales\n",
    "\n",
    "WITH product_sales AS (\n",
    "  -- 1) total sales per product\n",
    "  SELECT\n",
    "    product_id,\n",
    "    SUM(sales) AS product_sales\n",
    "  FROM orders\n",
    "  GROUP BY product_id\n",
    "),\n",
    "\n",
    "sales_with_cumulative AS (\n",
    "  -- 2) compute cumulative product_sales (highest -> lowest) and the 80% threshold\n",
    "  SELECT\n",
    "    product_id,\n",
    "    product_sales,\n",
    "    -- running (cumulative) sum ordered by product_sales descending\n",
    "    SUM(product_sales) OVER (ORDER BY product_sales DESC\n",
    "                             ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_sales,\n",
    "    -- total sales * 0.8 (same value repeated for every row)\n",
    "    0.8 * SUM(product_sales) OVER () AS threshold_80pct\n",
    "  FROM product_sales\n",
    ")\n",
    "\n",
    "-- 3) pick products until cumulative sales reach <= 80% of total sales\n",
    "SELECT\n",
    "  product_id,\n",
    "  product_sales,\n",
    "  running_sales,\n",
    "  threshold_80pct\n",
    "FROM sales_with_cumulative\n",
    "WHERE running_sales <= threshold_80pct\n",
    "ORDER BY product_sales DESC;\n",
    "```\n",
    "\n",
    "Key improvements:\n",
    "\n",
    "* Named CTEs and columns clearly.\n",
    "* Explicit `ROWS BETWEEN` clause added to make running-window intent clear.\n",
    "* `threshold_80pct` naming clarifies purpose.\n",
    "* Final `ORDER BY product_sales DESC` for readability.\n",
    "\n",
    "---\n",
    "\n",
    "# 8) Convert the SQL into equivalent PySpark DataFrame code\n",
    "\n",
    "Below is PySpark DataFrame code that reproduces the SQL logic. I assume you already have a Spark `DataFrame` named `orders_df` with at least `product_id` and `sales` columns.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1) Aggregate to product-level sales\n",
    "product_sales_df = (\n",
    "    orders_df\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(F.sum(\"sales\").alias(\"product_sales\"))\n",
    ")\n",
    "\n",
    "# 2) Define windows:\n",
    "#    - window_running: order by product_sales descending, cumulative from start to current row\n",
    "#    - window_total: no partition, used to compute total across all rows\n",
    "window_running = Window.orderBy(F.desc(\"product_sales\")).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "window_total = Window.partitionBy()  # partitionBy() with no columns = global window\n",
    "\n",
    "# 3) Add running cumulative and 80% threshold\n",
    "sales_with_cum_df = (\n",
    "    product_sales_df\n",
    "    .withColumn(\"running_sales\", F.sum(\"product_sales\").over(window_running))\n",
    "    .withColumn(\"threshold_80pct\", 0.8 * F.sum(\"product_sales\").over(window_total))\n",
    ")\n",
    "\n",
    "# 4) Filter rows where running_sales <= threshold_80pct\n",
    "top_80pct_products_df = (\n",
    "    sales_with_cum_df\n",
    "    .filter(F.col(\"running_sales\") <= F.col(\"threshold_80pct\"))\n",
    "    .orderBy(F.desc(\"product_sales\"))\n",
    ")\n",
    "\n",
    "# top_80pct_products_df now matches the SQL result\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9) Explain the PySpark code line-by-line\n",
    "\n",
    "1. `from pyspark.sql import functions as F`\n",
    "\n",
    "   * Short alias for commonly used Spark SQL functions.\n",
    "\n",
    "2. `from pyspark.sql.window import Window`\n",
    "\n",
    "   * Import to create window specifications for window functions.\n",
    "\n",
    "3. `product_sales_df = (orders_df.groupBy(\"product_id\").agg(F.sum(\"sales\").alias(\"product_sales\")))`\n",
    "\n",
    "   * Groups `orders_df` by `product_id` and computes `product_sales` (sum of sales per product). Equivalent to the first SQL CTE.\n",
    "\n",
    "4. `window_running = Window.orderBy(F.desc(\"product_sales\")).rowsBetween(Window.unboundedPreceding, Window.currentRow)`\n",
    "\n",
    "   * Defines a window that orders rows by `product_sales` descending and specifies the frame for cumulative sums: from the first row in the partition to the current row (running cumulative).\n",
    "\n",
    "5. `window_total = Window.partitionBy()`\n",
    "\n",
    "   * A global window (no partition columns) so that `sum(product_sales).over(window_total)` returns the grand total repeated for every row.\n",
    "\n",
    "6. `.withColumn(\"running_sales\", F.sum(\"product_sales\").over(window_running))`\n",
    "\n",
    "   * Adds a column `running_sales` containing cumulative sum of `product_sales` up to the current row in the descending order.\n",
    "\n",
    "7. `.withColumn(\"threshold_80pct\", 0.8 * F.sum(\"product_sales\").over(window_total))`\n",
    "\n",
    "   * Adds a column `threshold_80pct` that is 80% of the global sum of `product_sales`. Constant across rows.\n",
    "\n",
    "8. `.filter(F.col(\"running_sales\") <= F.col(\"threshold_80pct\"))`\n",
    "\n",
    "   * Keeps only products where the cumulative sum is ≤ threshold (same logic as SQL `WHERE`).\n",
    "\n",
    "9. `.orderBy(F.desc(\"product_sales\"))`\n",
    "\n",
    "   * Orders the final output with highest-selling products first (matches how the cumulative was computed).\n",
    "\n",
    "**Note about collect()**: I avoided using `.collect()` to retrieve total into driver; instead I computed grand total via a window (`Window.partitionBy()`), which keeps everything in the distributed execution and avoids driver roundtrips. That matches the SQL approach.\n",
    "\n",
    "---\n",
    "\n",
    "# 10) How thinking/process differs between SQL and PySpark\n",
    "\n",
    "* **Declarative vs. Imperative pipeline**\n",
    "\n",
    "  * SQL: you describe *what* you want (aggregations, window computations) and the engine decides execution. You write queries and CTEs to express transformations.\n",
    "  * PySpark: you build a transformation pipeline step-by-step, chaining DataFrame operations. It looks more imperative but still lazy (Spark optimizes later).\n",
    "\n",
    "* **Windows & frames**\n",
    "\n",
    "  * SQL window syntax is compact; PySpark requires constructing `Window` objects and using them with `over()`. Conceptually identical but syntactically different.\n",
    "\n",
    "* **Intermediate results**\n",
    "\n",
    "  * SQL uses CTEs for readability; PySpark uses intermediate DataFrames (variables). Both help readability, but in PySpark you explicitly name and reuse DataFrames.\n",
    "\n",
    "* **Global totals**\n",
    "\n",
    "  * In SQL you can use `SUM(...) OVER()` or a subquery. In PySpark you can either compute constants via `.agg(...).collect()` or use `Window.partitionBy()` to get global totals without driver roundtrip. Using a window keeps computation distributed.\n",
    "\n",
    "* **Tie-breaking and deterministic ordering**\n",
    "\n",
    "  * SQL ordering ties can be ambiguous without additional columns. Always think about tie-breakers when exact deterministic behavior is required. Same in PySpark: add more columns to `orderBy()` window if needed.\n",
    "\n",
    "* **Execution & debugging**\n",
    "\n",
    "  * SQL returns results immediately (on query run). In PySpark, operations are lazy until an action triggers execution (e.g., `.show()`, `.collect()`). For debugging, PySpark often requires small `.show()` checks of intermediate DataFrames.\n",
    "\n",
    "---\n",
    "\n",
    "# 11) Hints to solve it without seeing the solution\n",
    "\n",
    "### 3 Basic Hints\n",
    "\n",
    "1. First aggregate by product to get the total sales per product — don’t try to do running sums on the raw `orders` table.\n",
    "2. Sort products from largest `product_sales` to smallest — cumulative sums only make sense on an ordered list.\n",
    "3. Compute the grand total once (or compute 80% of total) and compare the running cumulative sum against that threshold.\n",
    "\n",
    "### 3 Advanced Hints\n",
    "\n",
    "1. Use a window function to compute the running (cumulative) sum. The frame should be “from start to current row” so you accumulate as you go.\n",
    "2. To compute the 80% threshold without a separate subquery, use a window with no partition (`OVER ()`) to get the global sum repeated on every row. Multiply it by 0.8.\n",
    "3. Think about ties: if multiple products have equal `product_sales`, decide whether to: (a) include all tied rows that keep you ≤80%, (b) include the first tied row that crosses the threshold, or (c) include the whole group. Implement tie-breaker logic explicitly if determinism matters.\n",
    "\n",
    "---\n",
    "\n",
    "# 12) Final SQL and final PySpark solutions (again)\n",
    "\n",
    "## Final SQL (clean, commented)\n",
    "\n",
    "```sql\n",
    "WITH product_sales AS (\n",
    "  SELECT product_id,\n",
    "         SUM(sales) AS product_sales\n",
    "  FROM orders\n",
    "  GROUP BY product_id\n",
    "),\n",
    "\n",
    "sales_with_cumulative AS (\n",
    "  SELECT\n",
    "    product_id,\n",
    "    product_sales,\n",
    "    SUM(product_sales) OVER (ORDER BY product_sales DESC\n",
    "                             ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_sales,\n",
    "    0.8 * SUM(product_sales) OVER () AS threshold_80pct\n",
    "  FROM product_sales\n",
    ")\n",
    "\n",
    "SELECT product_id,\n",
    "       product_sales,\n",
    "       running_sales,\n",
    "       threshold_80pct\n",
    "FROM sales_with_cumulative\n",
    "WHERE running_sales <= threshold_80pct\n",
    "ORDER BY product_sales DESC;\n",
    "```\n",
    "\n",
    "> If you prefer to **ensure at least 80% coverage** (i.e., include the product that first makes cumulative >= 80%), change the filter to:\n",
    "> `WHERE running_sales - product_sales < threshold_80pct`\n",
    "> This selects products whose cumulative before adding current product is < threshold — then you can include the product that crosses the threshold (or use `running_sales >= threshold_80pct` depending on exact inclusion rule).\n",
    "\n",
    "---\n",
    "\n",
    "## Final PySpark DataFrame code\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# assume orders_df exists with columns: product_id, sales\n",
    "\n",
    "# 1) product level aggregation\n",
    "product_sales_df = (\n",
    "    orders_df\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(F.sum(\"sales\").alias(\"product_sales\"))\n",
    ")\n",
    "\n",
    "# 2) windows for cumulative and global total\n",
    "window_running = Window.orderBy(F.desc(\"product_sales\")).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "window_total   = Window.partitionBy()  # global window\n",
    "\n",
    "# 3) add cumulative and threshold columns\n",
    "sales_with_cum_df = (\n",
    "    product_sales_df\n",
    "    .withColumn(\"running_sales\", F.sum(\"product_sales\").over(window_running))\n",
    "    .withColumn(\"threshold_80pct\", 0.8 * F.sum(\"product_sales\").over(window_total))\n",
    ")\n",
    "\n",
    "# 4) filter to top products that cumulatively add up to <= 80% of total sales\n",
    "top_80pct_products_df = (\n",
    "    sales_with_cum_df\n",
    "    .filter(F.col(\"running_sales\") <= F.col(\"threshold_80pct\"))\n",
    "    .orderBy(F.desc(\"product_sales\"))\n",
    ")\n",
    "\n",
    "# Show result\n",
    "top_80pct_products_df.show(truncate=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 13) Final teaching notes — how to generalize and apply this approach yourself\n",
    "\n",
    "* Pattern to remember: `aggregate -> sort -> cumulative -> threshold filter`.\n",
    "* Window functions are your friend for running totals and global totals (`SUM(...) OVER (...)`).\n",
    "* Be explicit about ordering and frames when computing cumulative sums to avoid accidental mistakes.\n",
    "* Consider tie-handling: if two products have equal sales, your ordering might be non-deterministic — add secondary keys (e.g., product_id) to `ORDER BY` in windows if you need reproducible results.\n",
    "* When implementing in distributed systems (Spark), prefer window-based total computations to avoid collecting large numbers to driver; use global windows (`Window.partitionBy()` with no columns) or compute aggregated scalars and broadcast if needed.\n",
    "* Always test with a small dataset that includes ties, single large product, many small products, and verify whether you want ≤80% or ≥80% semantics.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1af797fe-c0ed-4a53-ae84-92cb4ae9cf46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8670157494746566,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Prob_07",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
