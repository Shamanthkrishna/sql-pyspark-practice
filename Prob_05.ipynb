{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18a55155-60a4-46bc-af89-20f8c907c312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pivot - Unpivot\n",
    "The problem is to handle employee compensation data stored in a long format where each component appears as a separate row, and to reshape it into a wide pivoted format with components as columns, while also enabling the reverse transformation by unpivoting the wide table back into the original long format for flexible analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e3b46ca-13c6-4d05-825e-3dfddea8c948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Switch to my Catalog\n",
    "USE CATALOG workspace;\n",
    "\n",
    "-- Create schema if not exists\n",
    "CREATE SCHEMA IF NOT EXISTS sql_pyspark_practice;\n",
    "\n",
    "-- Use this schema\n",
    "USE sql_pyspark_practice;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42092c63-f0fc-4652-800f-ced794f049c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table emp_compensation (\n",
    "emp_id int,\n",
    "salary_component_type varchar(20),\n",
    "val int\n",
    ");\n",
    "\n",
    "insert into emp_compensation\n",
    "values (1,'salary',10000),(1,'bonus',5000),(1,'hike_percent',10)\n",
    ", (2,'salary',15000),(2,'bonus',7000),(2,'hike_percent',8)\n",
    ", (3,'salary',12000),(3,'bonus',6000),(3,'hike_percent',7);\n",
    "\n",
    "select * from emp_compensation;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be8d719-7371-4f18-82e9-df27baf1c161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f9e0cd0-4c80-4a3f-ab38-3defb3b7a42a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQL Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2a4b5d4-0f8d-4aa0-9b99-7c8f63ed25ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table emp_compensation_pivot as \n",
    "select emp_id,\n",
    "        sum(case when salary_component_type = 'salary' then val end) as salary,\n",
    "        sum(case when salary_component_type = 'bonus' then val end) as bonus,\n",
    "        sum(case when salary_component_type = 'hike_percent' then val end) as hike_percent\n",
    "from emp_compensation\n",
    "group by emp_id;\n",
    "\n",
    "select * from emp_compensation_pivot;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fb5d61d-2442-45ef-bfae-5b2684c77c39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420c9ee9-ce83-478e-84db-e34a2c03705d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = spark.table(\"emp_compensation\")\n",
    "display(df)\n",
    "\n",
    "\n",
    "df_pivot = df.groupBy(\"emp_id\").pivot(\"salary_component_type\").agg(sum(\"val\"))\n",
    "\n",
    "df_final = df_pivot.select(\n",
    "    col(\"emp_id\"),\n",
    "    col(\"salary\"),\n",
    "    col(\"bonus\"),\n",
    "    col(\"hike_percent\")\n",
    ")\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd1f4bfc-f264-4729-afc2-2734656e140f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unpivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03de8f0a-3bdf-41a5-9b2c-2b9359f5471e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQL Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bced3b6a-c3a6-47d5-b8ef-0e3efc97e3b5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763104340685}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select emp_id, 'salary' as salaray_component_type, salary as val from emp_compensation_pivot\n",
    "union all\n",
    "select emp_id, 'bonus' as salaray_component_type, bonus as val from emp_compensation_pivot\n",
    "union all\n",
    "select emp_id, 'hike_percent' as salaray_component_type, hike_percent as val from emp_compensation_pivot;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1935d748-7c70-41dc-8c61-5f409392eb1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "648162ed-0fc0-4034-8ea5-962981829642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_unpivot = df_final.selectExpr(\"emp_id\", \"stack(3, 'salary', salary, 'bonus', bonus, 'hike_percent', hike_percent) as (salary_component_type, val)\")\n",
    "display(df_unpivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d904bfe4-84bc-45c6-9f9a-12af48a42d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Learnings\n",
    "\n",
    "### âœ… **Outcome**\n",
    "\n",
    "* Successfully converted **tall/long data â†’ wide format (pivot)** and then **wide â†’ long format (unpivot)** using both **SQL** and **PySpark**.\n",
    "* Got equivalent results in SQL tables and PySpark DataFrames.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **What I Learnt**\n",
    "\n",
    "* How to **pivot** data in SQL using CASE + GROUP BY.\n",
    "* How to **pivot** in PySpark using `groupBy().pivot().agg()`.\n",
    "* How to **unpivot** in SQL using `UNION ALL`.\n",
    "* How to **unpivot** in PySpark using `stack()`.\n",
    "* Understanding of data reshaping between **row-based** and **column-based** formats.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸªœ **Steps Done**\n",
    "\n",
    "1. Created base table `emp_compensation` (long format).\n",
    "2. Built a pivoted table using:\n",
    "\n",
    "   * SQL CASE statements\n",
    "   * PySpark pivot function\n",
    "3. Built an unpivoted version using:\n",
    "\n",
    "   * SQL `UNION ALL`\n",
    "   * PySpark `stack()` function\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“˜ **Main Topics Covered (SQL + PySpark)**\n",
    "\n",
    "* **SQL**\n",
    "\n",
    "  * `GROUP BY`\n",
    "  * Conditional aggregation (`CASE WHEN`)\n",
    "  * `UNION ALL`\n",
    "  * Table creation\n",
    "  * PIVOT/UNPIVOT logic manually\n",
    "\n",
    "* **PySpark**\n",
    "\n",
    "  * DataFrame creation using `spark.table()`\n",
    "  * `groupBy()`\n",
    "  * `pivot()`\n",
    "  * `agg(sum())`\n",
    "  * `selectExpr()`\n",
    "  * `stack()` for unpivot\n",
    "  * Column selection with `col()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "498dc9e9-9694-4192-9e95-72c41a935913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# âœ… **1) Deeply analyze the SQL code**\n",
    "\n",
    "Your SQL contains **two parts**:\n",
    "\n",
    "### **(A) Pivot**\n",
    "\n",
    "Transforms rows â†’ columns.\n",
    "\n",
    "It takes this input structure:\n",
    "\n",
    "```\n",
    "emp_id | salary_component_type | val\n",
    "```\n",
    "\n",
    "And converts it into:\n",
    "\n",
    "```\n",
    "emp_id | salary | bonus | hike_percent\n",
    "```\n",
    "\n",
    "This is done using:\n",
    "\n",
    "```sql\n",
    "sum(case when salary_component_type = 'salary' then val end)\n",
    "```\n",
    "\n",
    "Classic PIVOT implementation.\n",
    "\n",
    "---\n",
    "\n",
    "### **(B) Unpivot**\n",
    "\n",
    "Transforms columns â†’ rows.\n",
    "\n",
    "It converts the pivoted table back to:\n",
    "\n",
    "```\n",
    "emp_id | salary_component_type | val\n",
    "```\n",
    "\n",
    "using:\n",
    "\n",
    "```sql\n",
    "select emp_id, 'salary' as salary_component_type, salary as val\n",
    "union all\n",
    "...\n",
    "```\n",
    "\n",
    "This is a classic UNPIVOT using `UNION ALL`.\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **2) Reconstructed original problem**\n",
    "\n",
    "Based on the SQL:\n",
    "\n",
    "> **Problem:**\n",
    "> Convert a compensation dataset from a tall/row-based format into a wide/pivoted format (salary, bonus, hike_percent as separate columns) **and then convert the wide table back into tall/unpivoted format**.\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **3) Explain the problem in plain English**\n",
    "\n",
    "You have employee compensation data stored like this:\n",
    "\n",
    "| emp_id | component_type | value |\n",
    "| ------ | -------------- | ----- |\n",
    "| 101    | salary         | 50000 |\n",
    "| 101    | bonus          | 5000  |\n",
    "| 101    | hike_percent   | 10    |\n",
    "\n",
    "But you want it like this:\n",
    "\n",
    "| emp_id | salary | bonus | hike_percent |\n",
    "| ------ | ------ | ----- | ------------ |\n",
    "| 101    | 50000  | 5000  | 10           |\n",
    "\n",
    "This is **pivoting**.\n",
    "\n",
    "And then the task asks you to convert that pivoted table back to the original long format.\n",
    "This is **unpivoting**.\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **4) Why the problem matters & what concepts it is testing**\n",
    "\n",
    "This question tests two core data transformation skills:\n",
    "\n",
    "### **ðŸ“Œ PIVOT**\n",
    "\n",
    "Turning row categories into columns.\n",
    "\n",
    "Used when:\n",
    "\n",
    "* Creating reporting tables.\n",
    "* Summarizing KPI components.\n",
    "* Creating ML feature tables.\n",
    "\n",
    "### **ðŸ“Œ UNPIVOT**\n",
    "\n",
    "Turning columns back into rows.\n",
    "\n",
    "Used when:\n",
    "\n",
    "* Normalizing data.\n",
    "* Reversing pivoted outputs.\n",
    "* Preparing for joins/analytics that expect row-level data.\n",
    "\n",
    "### **Skills tested**\n",
    "\n",
    "* CASE-based pivoting\n",
    "* Aggregation with GROUP BY\n",
    "* UNION ALL-based unpivoting\n",
    "* Understanding wide vs long data models\n",
    "* Schema transformation thinking\n",
    "\n",
    "This is extremely common in analytics, SQL interviews, ETL, and data engineering pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **5) How to logically think to solve such problems (general methodology)**\n",
    "\n",
    "### **PIVOT thinking**\n",
    "\n",
    "1. Identify **grouping column** â†’ `emp_id`\n",
    "2. Identify **pivot category column** â†’ `salary_component_type`\n",
    "3. Identify **value column** â†’ `val`\n",
    "4. For each category, create:\n",
    "\n",
    "   ```\n",
    "   sum(case when component = 'salary' then val end) as salary\n",
    "   ```\n",
    "5. Group by the base key (emp_id)\n",
    "\n",
    "### **UNPIVOT thinking**\n",
    "\n",
    "1. Identify each column you want to convert back into rows.\n",
    "2. For each column, create a SELECT block:\n",
    "\n",
    "   ```\n",
    "   select emp_id, 'salary' as type, salary as val\n",
    "   ```\n",
    "3. Combine using `UNION ALL`.\n",
    "\n",
    "Thinking pattern:\n",
    "\n",
    "* Pivot = CASE WHEN per category\n",
    "* Unpivot = UNION ALL per column\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **6) Step-by-step breakdown of the SQL code**\n",
    "\n",
    "---\n",
    "\n",
    "## **PIVOT section**\n",
    "\n",
    "### **Step 1 â€” Create pivot table**\n",
    "\n",
    "```sql\n",
    "create or replace table emp_compensation_pivot as\n",
    "select \n",
    "   emp_id,\n",
    "   sum(case when salary_component_type = 'salary' then val end) as salary,\n",
    "   sum(case when salary_component_type = 'bonus' then val end) as bonus,\n",
    "   sum(case when salary_component_type = 'hike_percent' then val end) as hike_percent\n",
    "from emp_compensation\n",
    "group by emp_id;\n",
    "```\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "* `emp_id` is grouped so each employee gets one row.\n",
    "* CASE picks values only when the component matches a type.\n",
    "* SUM is used to aggregate; safe even if there is only 1 value.\n",
    "* Result: pivoted data (wide format).\n",
    "\n",
    "---\n",
    "\n",
    "## **Unpivot section**\n",
    "\n",
    "```sql\n",
    "select emp_id, 'salary' as salary_component_type, salary as val \n",
    "from emp_compensation_pivot\n",
    "union all\n",
    "select emp_id, 'bonus', bonus \n",
    "from emp_compensation_pivot\n",
    "union all\n",
    "select emp_id, 'hike_percent', hike_percent \n",
    "from emp_compensation_pivot;\n",
    "```\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "* Each SELECT converts one column into rows.\n",
    "* `'salary'` is the type label.\n",
    "* `salary` is the value.\n",
    "* Repeated for bonus and hike_percent.\n",
    "* UNION ALL stacks them into a long table.\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **7) Clean, readable SQL with comments**\n",
    "\n",
    "### **PIVOT**\n",
    "\n",
    "```sql\n",
    "-- Pivot employee compensation by component type\n",
    "CREATE OR REPLACE TABLE emp_compensation_pivot AS\n",
    "SELECT \n",
    "    emp_id,\n",
    "\n",
    "    -- Pick salary values\n",
    "    SUM(CASE WHEN salary_component_type = 'salary' THEN val END) AS salary,\n",
    "\n",
    "    -- Pick bonus values\n",
    "    SUM(CASE WHEN salary_component_type = 'bonus' THEN val END) AS bonus,\n",
    "\n",
    "    -- Pick hike percentage values\n",
    "    SUM(CASE WHEN salary_component_type = 'hike_percent' THEN val END) AS hike_percent\n",
    "\n",
    "FROM emp_compensation\n",
    "GROUP BY emp_id;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **UNPIVOT**\n",
    "\n",
    "```sql\n",
    "-- Unpivot the wide table back into long form\n",
    "SELECT emp_id, 'salary' AS salary_component_type, salary AS val\n",
    "FROM emp_compensation_pivot\n",
    "UNION ALL\n",
    "\n",
    "SELECT emp_id, 'bonus', bonus\n",
    "FROM emp_compensation_pivot\n",
    "UNION ALL\n",
    "\n",
    "SELECT emp_id, 'hike_percent', hike_percent\n",
    "FROM emp_compensation_pivot;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **8) Equivalent PySpark DataFrame code**\n",
    "\n",
    "Assume:\n",
    "\n",
    "```python\n",
    "df = spark.table(\"emp_compensation\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **PIVOT in PySpark**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "pivot_df = (\n",
    "    df\n",
    "    .groupBy(\"emp_id\")\n",
    "    .pivot(\"salary_component_type\", [\"salary\", \"bonus\", \"hike_percent\"])\n",
    "    .agg(F.sum(\"val\"))\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **UNPIVOT in PySpark**\n",
    "\n",
    "PySpark has no built-in `unpivot`, so we manually stack using `unionByName`.\n",
    "\n",
    "```python\n",
    "salary_df = pivot_df.select(\n",
    "    \"emp_id\",\n",
    "    F.lit(\"salary\").alias(\"salary_component_type\"),\n",
    "    F.col(\"salary\").alias(\"val\")\n",
    ")\n",
    "\n",
    "bonus_df = pivot_df.select(\n",
    "    \"emp_id\",\n",
    "    F.lit(\"bonus\").alias(\"salary_component_type\"),\n",
    "    F.col(\"bonus\").alias(\"val\")\n",
    ")\n",
    "\n",
    "hike_df = pivot_df.select(\n",
    "    \"emp_id\",\n",
    "    F.lit(\"hike_percent\").alias(\"salary_component_type\"),\n",
    "    F.col(\"hike_percent\").alias(\"val\")\n",
    ")\n",
    "\n",
    "unpivot_df = salary_df.unionByName(bonus_df).unionByName(hike_df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **9) Line-by-line Explanation of the PySpark Code**\n",
    "\n",
    "### **Pivot**\n",
    "\n",
    "```python\n",
    "df.groupBy(\"emp_id\")\n",
    "```\n",
    "\n",
    "Group by employee.\n",
    "\n",
    "```python\n",
    ".pivot(\"salary_component_type\", [\"salary\", \"bonus\", \"hike_percent\"])\n",
    "```\n",
    "\n",
    "Makes three new columns based on these category names.\n",
    "\n",
    "```python\n",
    ".agg(F.sum(\"val\"))\n",
    "```\n",
    "\n",
    "Aggregates values for each category.\n",
    "\n",
    "---\n",
    "\n",
    "### **Unpivot**\n",
    "\n",
    "Each section:\n",
    "\n",
    "```python\n",
    "salary_df = pivot_df.select(\n",
    "    \"emp_id\",\n",
    "    F.lit(\"salary\").alias(\"salary_component_type\"),\n",
    "    F.col(\"salary\").alias(\"val\")\n",
    ")\n",
    "```\n",
    "\n",
    "* Takes the salary column,\n",
    "* Adds the label \"salary\",\n",
    "* Renames it to val.\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "unionByName()\n",
    "```\n",
    "\n",
    "Stacks the DataFrames vertically.\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **10) How SQL thinking differs from PySpark thinking**\n",
    "\n",
    "| Concept        | SQL                | PySpark                                           |\n",
    "| -------------- | ------------------ | ------------------------------------------------- |\n",
    "| Pivot          | Use CASE/aggregate | Use `.pivot()`                                    |\n",
    "| Unpivot        | Use `UNION ALL`    | Explicit multiple DataFrame selects + unionByName |\n",
    "| Code style     | Declarative        | Transformation pipeline                           |\n",
    "| NULL handling  | Automatic          | Must understand column operations                 |\n",
    "| Data structure | Tables             | DataFrames                                        |\n",
    "\n",
    "PySpark requires more manual work for unpivoting.\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **11) Hints (3 basic + 3 advanced)**\n",
    "\n",
    "### **Basic Hints**\n",
    "\n",
    "1. Pivoting = CASE WHEN + SUM.\n",
    "2. Group by the main entity (emp_id).\n",
    "3. Unpivoting = multiple SELECTs stacked with UNION ALL.\n",
    "\n",
    "### **Advanced Hints**\n",
    "\n",
    "1. Always ensure missing values become NULL â€” SUM avoids issues.\n",
    "2. When unpivoting, make sure column names match for UNION.\n",
    "3. Think in terms of long â†’ wide â†’ long transformations used in analytics pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **12) Final SQL + PySpark Solutions**\n",
    "\n",
    "### **Final SQL**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE emp_compensation_pivot AS\n",
    "SELECT \n",
    "    emp_id,\n",
    "    SUM(CASE WHEN salary_component_type = 'salary' THEN val END) AS salary,\n",
    "    SUM(CASE WHEN salary_component_type = 'bonus' THEN val END) AS bonus,\n",
    "    SUM(CASE WHEN salary_component_type = 'hike_percent' THEN val END) AS hike_percent\n",
    "FROM emp_compensation\n",
    "GROUP BY emp_id;\n",
    "\n",
    "SELECT emp_id, 'salary' AS salary_component_type, salary AS val \n",
    "FROM emp_compensation_pivot\n",
    "UNION ALL\n",
    "SELECT emp_id, 'bonus', bonus \n",
    "FROM emp_compensation_pivot\n",
    "UNION ALL\n",
    "SELECT emp_id, 'hike_percent', hike_percent \n",
    "FROM emp_compensation_pivot;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Final PySpark**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Pivot\n",
    "pivot_df = (\n",
    "    df.groupBy(\"emp_id\")\n",
    "      .pivot(\"salary_component_type\", [\"salary\", \"bonus\", \"hike_percent\"])\n",
    "      .agg(F.sum(\"val\"))\n",
    ")\n",
    "\n",
    "# Unpivot\n",
    "salary_df = pivot_df.select(\"emp_id\", F.lit(\"salary\").alias(\"salary_component_type\"), F.col(\"salary\").alias(\"val\"))\n",
    "bonus_df  = pivot_df.select(\"emp_id\", F.lit(\"bonus\").alias(\"salary_component_type\"), F.col(\"bonus\").alias(\"val\"))\n",
    "hike_df   = pivot_df.select(\"emp_id\", F.lit(\"hike_percent\").alias(\"salary_component_type\"), F.col(\"hike_percent\").alias(\"val\"))\n",
    "\n",
    "unpivot_df = salary_df.unionByName(bonus_df).unionByName(hike_df)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **13) Final Teaching Takeaway**\n",
    "\n",
    "For **pivoting**, think:\n",
    "\n",
    "> \"For each category, pull its value into a separate column using CASE.\"\n",
    "\n",
    "For **unpivoting**, think:\n",
    "\n",
    "> \"For each column, create a row with a type label and a value, then UNION them.\"\n",
    "\n",
    "This pivotâ€“unpivot pattern is extremely common in SQL interviews and ETL systems.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5723337425000115,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Prob_05",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
